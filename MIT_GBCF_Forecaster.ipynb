{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current commit:  2020-10-04 00:50:49  :  Automated update \n",
      "Current commit:  2020-10-04 00:50:49  :  Automated update \n"
     ]
    }
   ],
   "source": [
    "import git\n",
    "import time\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta, date\n",
    "import io\n",
    "\n",
    "REPO_DIR_NAME = \"/Users/saksun/work/COVID-19\"\n",
    "REMOTE_URL = \"https://github.com/CSSEGISandData/COVID-19.git\"\n",
    "start_date = datetime(2020, 10, 2)\n",
    "end_date = datetime(2020, 10, 4)\n",
    "predictionFilePath= \"data/predictions/predicted_us_deaths_2020-09-27.csv\"\n",
    "skip_day_predictions =  255\n",
    "model_day_offset=236\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "def init():\n",
    "    repo = git.Repo.init(REPO_DIR_NAME)    \n",
    "    repo.remotes[0].fetch()\n",
    "    repo.remotes[0].pull()\n",
    "\n",
    "def is_exists(filename, sha):\n",
    "    repo = git.Repo(REPO_DIR_NAME)\n",
    "    \"\"\"Check if a file in current commit exist.\"\"\"\n",
    "    files = repo.git.show(\"--pretty=\", \"--name-only\", sha)\n",
    "    if filename in files:\n",
    "        return True\n",
    "    \n",
    "def get_file_commits(filename, commits):\n",
    "    file_commits = []\n",
    "    for commit in commits:\n",
    "        if is_exists(filename, commit.hexsha):\n",
    "            file_commits.append(commit)\n",
    "    return file_commits\n",
    "\n",
    "def getDataFrame(filename):\n",
    "    repo = git.Repo(REPO_DIR_NAME)\n",
    "    commits = list(repo.iter_commits(\"master\"))\n",
    "    covid_file_commits = get_file_commits(filename, commits)\n",
    "\n",
    "\n",
    "    current_time =   datetime(2020, 1, 1)\n",
    "    prev_commit_datetime = current_time\n",
    "    prev_commit = covid_file_commits[0]    \n",
    "    i = 0\n",
    "    fipsArray =[]\n",
    "    for file_commit in reversed(covid_file_commits):\n",
    "        file_commit = repo.commit(file_commit.hexsha)\n",
    "        commit_datetime_str = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(file_commit.committed_date))\n",
    "        commit_datetime = datetime.strptime(commit_datetime_str, '%Y-%m-%d %H:%M:%S')\n",
    "        \n",
    "        if  (((commit_datetime.date() != prev_commit_datetime.date())  \n",
    "             and prev_commit_datetime.date() > start_date.date())\n",
    "             and (commit_datetime.date() <= end_date.date())) :\n",
    "            \n",
    "            if commit_datetime.date() == date.today():\n",
    "                prev_commit= file_commit\n",
    "                prev_commit_datetime = commit_datetime\n",
    "                \n",
    "            print(\"Current commit: \", prev_commit_datetime,\" : \",\n",
    "                 prev_commit.message.replace('\\n', ' ') )\n",
    "            # Retrieve a file from the commit tree\n",
    "            # You can use the path helper to get the file by filename \n",
    "            targetfile = prev_commit.tree / filename\n",
    "            \n",
    "            with io.BytesIO(targetfile.data_stream.read()) as f:\n",
    "                if i == 0:\n",
    "                    US_Covid_df = pd.read_csv(f)\n",
    "                    #US_Covid_df = US_Covid_df.drop(US_Covid_df[US_Covid_df[\"Province_State\"] == \"Puerto Rico\"].index)\n",
    "                    \n",
    "                    #US_Covid_df.reset_index(drop=True, inplace=True)\n",
    "                    i =  i + 1\n",
    "                else:\n",
    "                    df = pd.read_csv(f)\n",
    "                    #df = df.drop(df[df[\"Province_State\"] == \"Puerto Rico\"].index)\n",
    "                    #df.reset_index(drop=True, inplace=True)\n",
    "                    dd = (prev_commit_datetime.date() -  timedelta(days=1)).strftime('%-m/%-d/%y')\n",
    "                    US_Covid_df[dd] = 0\n",
    "                    for index, row in US_Covid_df.iterrows():\n",
    "                        val = df.loc[df['FIPS'] == row[\"FIPS\"]][dd]\n",
    "                        if type(val) is int:\n",
    "                            US_Covid_df.at[index,dd] = val\n",
    "                        else:\n",
    "                            for v in val.values:\n",
    "                                US_Covid_df.at[index,dd] = v\n",
    "                        \n",
    "                    \n",
    "        prev_commit= file_commit\n",
    "        prev_commit_datetime = commit_datetime\n",
    "    US_Covid_df = US_Covid_df.reset_index()\n",
    "    return US_Covid_df\n",
    "init()\n",
    "#Read US Covid deaths\n",
    "US_Deaths_df_all = getDataFrame('csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_US.csv')\n",
    "#Read US Covid cases\n",
    "US_Cases_df_all = getDataFrame('csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_US.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_day: 255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/pandas/core/frame.py:4125: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().rename(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_day: 277\n"
     ]
    }
   ],
   "source": [
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import  plotly.express as px \n",
    "import plotly.graph_objects as go\n",
    "\n",
    "US_Deaths_df = US_Deaths_df_all.copy()\n",
    "US_Cases_df = US_Cases_df_all.copy()\n",
    "#US_Deaths_url = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_US.csv'\n",
    "#US_Deaths_df = pd.read_csv(US_Deaths_url, error_bad_lines=True)\n",
    "US_Deaths_df=US_Deaths_df.drop(columns=['index','UID','iso2','iso3','code3','Admin2', 'Country_Region','Lat','Long_','Combined_Key'])\n",
    "US_Deaths_df=US_Deaths_df.fillna(0)\n",
    "US_Deaths_df=US_Deaths_df.melt(id_vars=[\"FIPS\",\"Population\",\"Province_State\"], \n",
    "        var_name=\"Date\", \n",
    "        value_name=\"Value\")\n",
    "\n",
    "#Confirmed_url = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_US.csv'\n",
    "#US_Cases_df = pd.read_csv(Confirmed_url, error_bad_lines=True)\n",
    "US_Cases_df=US_Cases_df.drop(columns=['index','UID','iso2','iso3','code3','Admin2', 'Province_State', 'Country_Region','Lat','Long_','Combined_Key'])\n",
    "US_Cases_df=US_Cases_df.fillna(0)#US_Cases_df.dropna(subset=['FIPS'])\n",
    "US_Cases_df=US_Cases_df.melt(id_vars=['FIPS'], \n",
    "        var_name=\"Date\", \n",
    "        value_name=\"Value\")\n",
    "\n",
    "    \n",
    "US_Cases_df.iloc[:, 1] = pd.to_datetime(US_Cases_df.iloc[:, 1])\n",
    "US_Cases_df.iloc[:, 1]  = (US_Cases_df.iloc[:, 1]  - US_Cases_df['Date'].iloc[0]).dt.days\n",
    "US_Cases_df = US_Cases_df.rename(columns={'Date': 'Days', 'Value':'Cumulative_Cases'})\n",
    "    \n",
    "\n",
    "US_Deaths_df.iloc[:, 3] = pd.to_datetime(US_Deaths_df.iloc[:, 3])\n",
    "US_Deaths_df.iloc[:, 3]  = (US_Deaths_df.iloc[:, 3]  - US_Deaths_df['Date'].iloc[0]).dt.days\n",
    "US_Deaths_df = US_Deaths_df.rename(columns={'Date': 'Days', 'Value':'Cumulative_Deaths' })\n",
    "print(\"max_day:\" , US_Deaths_df['Days'].max())\n",
    "\n",
    "US_Predicted_Deaths = pd.read_csv(predictionFilePath,usecols=['FIPS','Population','State', 'Days','Predicted_Cumulative_Deaths']) \n",
    "US_Predicted_Deaths_Future = US_Predicted_Deaths[US_Predicted_Deaths[\"Days\"] > skip_day_predictions]\n",
    "US_Predicted_Deaths_Future.rename(columns = {'State':'Province_State', \n",
    "                                      'Predicted_Cumulative_Deaths':'Cumulative_Deaths'}, inplace = True)\n",
    "US_Deaths_df = pd.concat([US_Deaths_df, US_Predicted_Deaths_Future])\n",
    "\n",
    "US_Predicted_Cases = pd.read_csv(predictionFilePath,usecols=['FIPS', 'Days','Predicted_Cumulative_Cases'])\n",
    "US_Predicted_Cases_Future = US_Predicted_Cases[US_Predicted_Cases[\"Days\"] > skip_day_predictions]\n",
    "US_Predicted_Cases_Future.rename(columns = { 'Predicted_Cumulative_Cases':'Cumulative_Cases'}, inplace = True)\n",
    "US_Cases_df = pd.concat([US_Cases_df, US_Predicted_Cases_Future])\n",
    "print(\"max_day:\" , US_Deaths_df['Days'].max())\n",
    "\n",
    "#for index, row in US_Predicted_Deaths.iterrows():\n",
    "#    print(index)\n",
    "#    US_Deaths_df = US_Deaths_df.append({\"FIPS\": row[\"FIPS\"], \"Population\":0, \"Province_State\": row[\"State\"], \"Days\": row[\"Days\"], \"Cumulative_Deaths\":row[\"Predicted_Cumulative_Deaths\"]}, ignore_index=True)\n",
    "#    US_Cases_df = US_Cases_df.append({\"FIPS\": row[\"FIPS\"], \"Days\": row[\"Days\"], \"Cumulative_Cases\" : row[\"Predicted_Cumulative_Cases\"]}, ignore_index=True) \n",
    "\n",
    "#US_Deaths_df['Weekly_Deaths'] = US_Deaths_df.groupby(['FIPS', 'Week'])['Cumulative_Deaths'].transform(lambda x: (x.iat[-1] - x.iat[0]))\n",
    "US_Deaths_df['Weekly_Deaths'] = 0\n",
    "US_Deaths_df['Weekly_Deaths_Per'] = 0.0\n",
    "US_Deaths_df['Past_Week_Cumulative_Deaths'] = 0\n",
    "US_Deaths_df = US_Deaths_df.reset_index()\n",
    "US_Deaths_df.sort_values(by=['FIPS','Days'], inplace=True)\n",
    "cdArray = US_Deaths_df['Cumulative_Deaths'].to_numpy()\n",
    "fipsArray = US_Deaths_df['FIPS'].to_numpy()\n",
    "i = 0\n",
    "j = 0\n",
    "FIPS = 0.0\n",
    "for index, row in US_Deaths_df.iterrows():\n",
    "  countyFIPS = row['FIPS']\n",
    "  if FIPS == countyFIPS: \n",
    "    if j > 6 and fipsArray[i-7] == countyFIPS :\n",
    "        previous_cum_deaths = cdArray[i-7]\n",
    "        weekly_deaths = row['Cumulative_Deaths'] - previous_cum_deaths\n",
    "        US_Deaths_df.at[index,'Weekly_Deaths'] =  weekly_deaths\n",
    "        if previous_cum_deaths  == 0:\n",
    "          previous_cum_deaths = 1\n",
    "        US_Deaths_df.at[index, 'Past_Week_Cumulative_Deaths'] = previous_cum_deaths\n",
    "        US_Deaths_df.at[index, 'Weekly_Deaths_Per'] = (weekly_deaths  * 1.00000) / (previous_cum_deaths * 1.00000)\n",
    "  else:\n",
    "    FIPS = countyFIPS\n",
    "    j = 0 \n",
    "    US_Deaths_df.at[index,'Weekly_Deaths'] = 0\n",
    "  i = i + 1\n",
    "  j = j + 1\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "US_Cases_df['Weekly_Cases'] = 0\n",
    "US_Cases_df['Weekly_Cases_Per'] = 0.0\n",
    "US_Cases_df['Past_Week_Cumulative_Cases'] = 0\n",
    "US_Cases_df = US_Cases_df.reset_index()\n",
    "US_Cases_df.sort_values(by=['FIPS','Days'], inplace=True)\n",
    "cdArray = US_Cases_df['Cumulative_Cases'].to_numpy()\n",
    "fipsArray = US_Cases_df['FIPS'].to_numpy()\n",
    "i = 0\n",
    "j = 0\n",
    "for index, row in US_Cases_df.iterrows():\n",
    "  countyFIPS = row['FIPS']\n",
    "  if FIPS == countyFIPS: \n",
    "    if j > 6 and fipsArray[i-7] == countyFIPS :\n",
    "        previous_cum_cases = cdArray[i-7]\n",
    "        weekly_cases = row['Cumulative_Cases'] - previous_cum_cases\n",
    "        US_Cases_df.at[index,'Weekly_Cases'] =  weekly_cases\n",
    "        if previous_cum_cases  == 0:\n",
    "          previous_cum_cases = 1\n",
    "        US_Cases_df.at[index, 'Past_Week_Cumulative_Cases'] = previous_cum_cases\n",
    "        US_Cases_df.at[index, 'Weekly_Cases_Per'] = (weekly_cases * 100.0) / (previous_cum_cases * 1.0)\n",
    "  else:\n",
    "    FIPS = countyFIPS\n",
    "    j = 0\n",
    "    US_Cases_df.at[index,'Weekly_Cases'] = 0\n",
    "  i = i + 1\n",
    "  j = j + 1\n",
    "    \n",
    "mobility = pd.read_csv('https://raw.githubusercontent.com/COVIDExposureIndices/COVIDExposureIndices/master/dex_data/county_dex.csv')\n",
    "mobility.iloc[:, 1] = pd.to_datetime(mobility.iloc[:, 1])\n",
    "mobility.iloc[:, 1]  = (mobility.iloc[:, 1]  - mobility['date'].iloc[0]).dt.days\n",
    "mobility = mobility.rename(columns={'date': 'Days'})\n",
    "mobility[\"Days\"] = mobility[\"Days\"] + 14\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-2c5a314e1a7d>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  US_Deaths_Tail_Rows[\"Days\"] = US_Deaths_Tail_Rows[\"Days\"] + 7\n",
      "<ipython-input-8-2c5a314e1a7d>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  US_Deaths_Tail_Rows[\"Past_Week_Cumulative_Deaths\"] = US_Deaths_Tail_Rows[\"Cumulative_Deaths\"]\n",
      "<ipython-input-8-2c5a314e1a7d>:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  US_Cases_Tail_Rows[\"Days\"] = US_Cases_Tail_Rows[\"Days\"] + 7\n",
      "<ipython-input-8-2c5a314e1a7d>:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  US_Cases_Tail_Rows[\"Past_Week_Cumulative_Cases\"] = US_Cases_Tail_Rows[\"Cumulative_Cases\"]\n"
     ]
    }
   ],
   "source": [
    "max_days = US_Deaths_df['Days'].max()    \n",
    "US_Deaths_Tail_Rows = US_Deaths_df[US_Deaths_df[\"Days\"] > max_days - 7]\n",
    "US_Deaths_Tail_Rows[\"Days\"] = US_Deaths_Tail_Rows[\"Days\"] + 7 \n",
    "US_Deaths_Tail_Rows[\"Past_Week_Cumulative_Deaths\"] = US_Deaths_Tail_Rows[\"Cumulative_Deaths\"] \n",
    "#US_Deaths_Tail_Rows[\"Weekly_Deaths\"] = 0\n",
    "#US_Deaths_Tail_Rows[\"Weekly_Deaths_Per\"] = 0\n",
    "US_Deaths_df = pd.concat([US_Deaths_Tail_Rows, US_Deaths_df])\n",
    "US_Deaths_df.sort_values(by=['FIPS','Days'], inplace=True)\n",
    "\n",
    "#max_days = US_Deaths_df['Days'].max()    \n",
    "#US_Deaths_Tail_Rows = US_Deaths_df[US_Deaths_df[\"Days\"] > max_days - 8]\n",
    "#US_Deaths_Tail_Rows[\"Days\"] = US_Deaths_Tail_Rows[\"Days\"] + 8\n",
    "#US_Deaths_Tail_Rows[\"Cumulative_Deaths\"] = 0\n",
    "#US_Deaths_Tail_Rows[\"Past_Week_Cumulative_Deaths\"] = 0\n",
    "#US_Deaths_Tail_Rows[\"Weekly_Deaths\"] = 0\n",
    "#US_Deaths_Tail_Rows[\"Weekly_Deaths_Per\"] = 0\n",
    "#US_Deaths_df = pd.concat([US_Deaths_Tail_Rows, US_Deaths_df])\n",
    "#US_Deaths_df.sort_values(by=['FIPS','Days'], inplace=True)\n",
    "\n",
    "\n",
    "max_days = US_Cases_df['Days'].max()    \n",
    "US_Cases_Tail_Rows = US_Cases_df[US_Cases_df[\"Days\"] > max_days - 7]\n",
    "US_Cases_Tail_Rows[\"Days\"] = US_Cases_Tail_Rows[\"Days\"] + 7\n",
    "US_Cases_Tail_Rows[\"Past_Week_Cumulative_Cases\"] = US_Cases_Tail_Rows[\"Cumulative_Cases\"]\n",
    "#US_Cases_Tail_Rows[\"Weekly_Cases\"] = 0\n",
    "#US_Cases_Tail_Rows[\"Weekly_Cases_Per\"] = 0\n",
    "US_Cases_df = pd.concat([US_Cases_Tail_Rows, US_Cases_df])\n",
    "US_Cases_df.sort_values(by=['FIPS','Days'], inplace=True)\n",
    "\n",
    "#max_days = US_Cases_df['Days'].max()    \n",
    "#US_Cases_Tail_Rows = US_Cases_df[US_Cases_df[\"Days\"] > max_days - 8]\n",
    "#US_Cases_Tail_Rows[\"Days\"] = US_Cases_Tail_Rows[\"Days\"] + 8\n",
    "#US_Cases_Tail_Rows[\"Cumulative_Cases\"] = 0\n",
    "#US_Cases_Tail_Rows[\"Past_Week_Cumulative_Cases\"] = 0\n",
    "#US_Cases_Tail_Rows[\"Weekly_Cases\"] = 0\n",
    "#US_Cases_Tail_Rows[\"Weekly_Cases_Per\"] = 0\n",
    "#US_Cases_df = pd.concat([US_Cases_Tail_Rows, US_Cases_df])\n",
    "#US_Cases_df.sort_values(by=['FIPS','Days'], inplace=True)\n",
    "\n",
    "\n",
    "US_Deaths_df.to_csv(\"usdf.csv\")\n",
    "US_Cases_df.to_csv(\"uscf.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RvHB7ubQ8LjS"
   },
   "outputs": [],
   "source": [
    "def daterange(date1, date2):\n",
    "    for n in range(int ((date2 - date1).days)+1):\n",
    "        yield date1 + timedelta(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kjOaGxmUwynr"
   },
   "outputs": [],
   "source": [
    "#load state tests data to combine with cases and deaths\n",
    "#data is not available for all days, populate zero for missing ones\n",
    "from datetime import timedelta, date\n",
    "\n",
    "state_codes = pd.read_csv('https://docs.google.com/spreadsheets/d/1DAqxIYJdagFN85ncoTQO-CbpJLQECvZWt6qwNMQZUkk/export?format=csv')\n",
    "start_date = date(2020, 1, 22)\n",
    "end_date = date.today()\n",
    "\n",
    "state_codes_date_df = pd.DataFrame({'Date': pd.Series([], dtype='str'),\n",
    "                   'Code': pd.Series([], dtype='str'),\n",
    "                   'State': pd.Series([], dtype='str')})\n",
    "for single_date in daterange(start_date, end_date):\n",
    "  for index, row in state_codes.iterrows():\n",
    "    state_codes_date_df = state_codes_date_df.append({'Date': single_date.strftime(\"%Y%m%d\"), 'Code' : row[\"Code\"], 'State' : row[\"State\"]}, ignore_index=True)\n",
    "state_tests = pd.read_csv(\"https://covidtracking.com/api/v1/states/daily.csv\")\n",
    "convert_dict = {'date': str} \n",
    "state_tests = state_tests.astype(convert_dict)\n",
    "state_tests_df = state_codes_date_df.merge(state_tests[['date','state','positive', 'negative']],how='left', left_on=['Date', 'Code'], right_on=['date', 'state'])\n",
    "state_tests_df.sort_values(['Date','State'], inplace=True)\n",
    "state_tests_df.iloc[:, 0] = pd.to_datetime(state_tests_df.iloc[:, 0],format=\"%Y%m%d\")\n",
    "state_tests_df.iloc[:, 0]  = (state_tests_df.iloc[:, 0] - state_tests_df['Date'].iloc[0]).dt.days\n",
    "state_tests_df = state_tests_df.rename(columns={'Date': 'Days'})\n",
    "state_tests_df['Cumulative_Tests'] = state_tests_df['positive'] + state_tests_df ['negative']\n",
    "state_tests_df = state_tests_df.drop(columns=['date', 'state', 'positive', 'negative'])\n",
    "state_tests_df.fillna(0, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "21GaGHXZT8M2"
   },
   "outputs": [],
   "source": [
    "#Read county variates dataset and combine with FIPS, race, icu_beds dataset\n",
    "county_covariates= pd.read_csv('https://raw.githubusercontent.com/JieYingWu/COVID-19_US_County-level_Summaries/master/data/counties.csv').dropna(subset=['FIPS'])\n",
    "county_covariates.FIPS = county_covariates.FIPS.astype('int64')\n",
    "county_covariates=county_covariates.set_index('FIPS')\n",
    "age_race_df = pd.read_csv('https://docs.google.com/spreadsheets/d/12GIRONjeNHeKFb3EKpo5r-VvdsnTwwe0iOtKBsnZVM0/export?format=csv')\n",
    "county_icu_beds= pd.read_csv('https://docs.google.com/spreadsheets/d/13iUBUwRcE91_x9FhsF8Ugcb0_tFauWJF2Z-PSkERDlU/export?format=csv')\n",
    "FIPS = pd.read_csv('https://docs.google.com/spreadsheets/d/1jUwRaTSJ__3Wp60cZLLox5u55mJTZrShtjEK4d7xTEY/export?format=csv')\n",
    "covariates = age_race_df.merge(county_covariates, how='inner', left_on=[\"fips\"], right_on=['FIPS'])\n",
    "covariates = covariates.drop(['STNAME', 'County', 'Unnamed: 0','State', 'Area_Name'], axis=1)\n",
    "covariates = covariates.merge(county_icu_beds, how='inner', left_on=[\"fips\"], right_on=['fips'])\n",
    "covariates = covariates.drop(['County','State'], axis=1)\n",
    "covariates = covariates.fillna(0)\n",
    "#dropping full NaN counties\n",
    "#covariates = covariates[covariates['Jul Temp Max / F'].notnull()]\n",
    "#merge with US FIPS and makue  sure FIPS are only in the US\n",
    "US_Deaths_All_FIPS_df= US_Deaths_df.copy()\n",
    "US_Cases_All_FIPS_df= US_Cases_df.copy()\n",
    "US_Deaths_df = US_Deaths_df.merge(FIPS,how='inner', left_on=[\"FIPS\"], right_on=['fips'])\n",
    "US_Cases_df = US_Cases_df.merge(FIPS,how='inner', left_on=[\"FIPS\"], right_on=['fips'])\n",
    "list(covariates.columns) \n",
    "covariates = covariates.dropna(axis='columns')\n",
    "list(covariates.columns) \n",
    "\n",
    "covariates=covariates[['fips',\n",
    " 'TOT_POP',\n",
    " '0-9',\n",
    " '0-9 y/o % of total pop',\n",
    " '10-19',\n",
    " '10-19 y/o % of total pop',\n",
    " '20-29',\n",
    " '20-29 y/o % of total pop',\n",
    " '30-39',\n",
    " '30-39 y/o % of total pop',\n",
    " '40-49',\n",
    " '40-49 y/o % of total pop',\n",
    " '50-59',\n",
    " '50-59 y/o % of total pop',\n",
    " '60-69',\n",
    " '60-69 y/o % of total pop',\n",
    " '70-79',\n",
    " '70-79 y/o % of total pop',\n",
    " '80+',\n",
    " '80+ y/o % of total pop',\n",
    " 'White-alone pop',\n",
    " '% White-alone',\n",
    " 'Black-alone pop',\n",
    " '% Black-alone',\n",
    " 'Native American/American Indian-alone pop',\n",
    " '% NA/AI-alone',\n",
    " 'Asian-alone pop',\n",
    " '% Asian-alone',\n",
    " 'Hawaiian/Pacific Islander-alone pop',\n",
    " '% Hawaiian/PI-alone',\n",
    " 'Two or more races pop',\n",
    " '% Two or more races',\n",
    " 'POP_ESTIMATE_2018',\n",
    " 'N_POP_CHG_2018',\n",
    " 'GQ_ESTIMATES_2018',\n",
    " 'R_birth_2018',\n",
    " 'R_death_2018',\n",
    " 'R_NATURAL_INC_2018',\n",
    " 'R_INTERNATIONAL_MIG_2018',\n",
    " 'R_DOMESTIC_MIG_2018',\n",
    " 'R_NET_MIG_2018',\n",
    " 'Less than a high school diploma 2014-18',\n",
    " 'High school diploma only 2014-18',\n",
    " \"Some college or associate's degree 2014-18\",\n",
    " \"Bachelor's degree or higher 2014-18\",\n",
    " 'Percent of adults with less than a high school diploma 2014-18',\n",
    " 'Percent of adults with a high school diploma only 2014-18',\n",
    " \"Percent of adults completing some college or associate's degree 2014-18\",\n",
    " \"Percent of adults with a bachelor's degree or higher 2014-18\",\n",
    " 'POVALL_2018',\n",
    " 'PCTPOVALL_2018',\n",
    " 'PCTPOV017_2018',\n",
    " 'PCTPOV517_2018',\n",
    " 'MEDHHINC_2018',\n",
    " 'CI90LBINC_2018',\n",
    " 'CI90UBINC_2018',\n",
    " 'Civilian_labor_force_2018',\n",
    " 'Employed_2018',\n",
    " 'Unemployed_2018',\n",
    " 'Unemployment_rate_2018',\n",
    " 'Median_Household_Income_2018',\n",
    " 'Med_HH_Income_Percent_of_State_Total_2018',\n",
    " 'Jan Precipitation / inch',\n",
    " 'Feb Precipitation / inch',\n",
    " 'Mar Precipitation / inch',\n",
    " 'Apr Precipitation / inch',\n",
    " 'May Precipitation / inch',\n",
    " 'Jun Precipitation / inch',\n",
    " 'Jul Precipitation / inch',\n",
    " 'Jan Temp AVG / F',\n",
    " 'Feb Temp AVG / F',\n",
    " 'Mar Temp AVG / F',\n",
    " 'Apr Temp AVG / F',\n",
    " 'May Temp AVG / F',\n",
    " 'Jun Temp AVG / F',\n",
    " 'Jul Temp AVG / F',\n",
    " 'Active Physicians per 100000 Population 2018 (AAMC)',\n",
    " 'Total Active Patient Care Physicians per 100000 Population 2018 (AAMC)',\n",
    " 'Active Primary Care Physicians per 100000 Population 2018 (AAMC)',\n",
    " 'Active Patient Care Primary Care Physicians per 100000 Population 2018 (AAMC)',\n",
    " 'Active General Surgeons per 100000 Population 2018 (AAMC)',\n",
    " 'Active Patient Care General Surgeons per 100000 Population 2018 (AAMC)',\n",
    " 'Total nurse practitioners (2019)',\n",
    " 'Total physician assistants (2019)',\n",
    " 'Total Hospitals (2019)',\n",
    " 'Internal Medicine Primary Care (2019)',\n",
    " 'Family Medicine/General Practice Primary Care (2019)',\n",
    " 'Total Specialist Physicians (2019)',\n",
    " 'ICU Beds_x',\n",
    " 'Total Population',\n",
    " 'Population Aged 60+',\n",
    " 'Percent of Population Aged 60+']]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VF6n1wU4ynx3"
   },
   "outputs": [],
   "source": [
    "#build prior - 2 ,3 week(s) cases and deaths as features\n",
    "\n",
    "US_Cases_Prior_Fourteen_df = US_Cases_df.copy() \n",
    "US_Cases_Prior_Fourteen_df[\"Days\"] = US_Cases_df[\"Days\"] + 14\n",
    "US_Cases_Prior_Fourteen_df.rename(columns = {'Weekly_Cases_Per':'Two_Week_Prior_Weekly_Cases_Per', 'Weekly_Cases':'Two_Week_Prior_Weekly_Cases'}, inplace = True)\n",
    "US_Cases_Prior_TwentyOne_df = US_Cases_df.copy() \n",
    "US_Cases_Prior_TwentyOne_df[\"Days\"] = US_Cases_df[\"Days\"] + 21\n",
    "US_Cases_Prior_TwentyOne_df.rename(columns = {'Weekly_Cases_Per':'Three_Week_Prior_Weekly_Cases_Per', 'Weekly_Cases':'Three_Week_Prior_Weekly_Cases'}, inplace = True)\n",
    "US_Deaths_Cases_df = US_Deaths_df.merge(US_Cases_df)\n",
    "US_Deaths_Cases_df = US_Deaths_Cases_df.merge(US_Cases_Prior_TwentyOne_df[['Three_Week_Prior_Weekly_Cases_Per', 'Three_Week_Prior_Weekly_Cases', 'FIPS', 'Days']], how='left', left_on=['FIPS', \"Days\"], right_on=['FIPS', 'Days'])\n",
    "US_Deaths_Cases_df = US_Deaths_Cases_df.merge(US_Cases_Prior_Fourteen_df[['Two_Week_Prior_Weekly_Cases_Per', 'Two_Week_Prior_Weekly_Cases', 'FIPS', 'Days']], how='left', left_on=['FIPS', \"Days\"], right_on=['FIPS', 'Days'])\n",
    "\n",
    "US_Deaths_Prior_TwentyOne_df = US_Deaths_df.copy()\n",
    "US_Deaths_Prior_TwentyOne_df[\"Days\"] = US_Deaths_df[\"Days\"] + 21\n",
    "US_Deaths_Prior_TwentyOne_df.rename(columns = {'Weekly_Deaths_Per':'Three_Week_Prior_Weekly_Deaths_Per', 'Weekly_Deaths':'Three_Week_Prior_Weekly_Deaths'}, inplace = True)\n",
    "US_Deaths_Cases_df = US_Deaths_Cases_df.merge(US_Deaths_Prior_TwentyOne_df[['Three_Week_Prior_Weekly_Deaths_Per', 'Three_Week_Prior_Weekly_Deaths', 'FIPS', 'Days']], how='left', left_on=['FIPS', \"Days\"], right_on=['FIPS', 'Days'])\n",
    "\n",
    "US_Deaths_Prior_Fourteen_df = US_Deaths_df.copy()\n",
    "US_Deaths_Prior_Fourteen_df[\"Days\"] = US_Deaths_df[\"Days\"] + 14\n",
    "US_Deaths_Prior_Fourteen_df.rename(columns = {'Weekly_Deaths_Per':'Two_Week_Prior_Weekly_Deaths_Per', 'Weekly_Deaths':'Two_Week_Prior_Weekly_Deaths'}, inplace = True)\n",
    "US_Deaths_Cases_df = US_Deaths_Cases_df.merge(US_Deaths_Prior_Fourteen_df[['Two_Week_Prior_Weekly_Deaths_Per', 'Two_Week_Prior_Weekly_Deaths', 'FIPS', 'Days']], how='left', left_on=['FIPS', \"Days\"], right_on=['FIPS', 'Days'])\n",
    "US_Deaths_Cases_df.to_csv(\"udc1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 439
    },
    "colab_type": "code",
    "id": "xseYJzjKY8FK",
    "outputId": "c77e79f3-b0e5-4328-ffca-afbd3fa0cb6e"
   },
   "outputs": [],
   "source": [
    "mobility[\"county\"] = mobility[\"county\"].astype(\"float\")\n",
    "US_Deaths_Cases_df = US_Deaths_Cases_df.merge(state_tests_df, how='left', left_on=['Province_State', \"Days\"], right_on=['State', 'Days'])\n",
    "US_Deaths_Cases_df = US_Deaths_Cases_df.merge(mobility[['dex', 'num_devices','county','Days']], how='left', left_on=['FIPS', \"Days\"], right_on=['county', 'Days'])\n",
    "\n",
    "covariates_merged = covariates.merge(US_Deaths_Cases_df[[\n",
    "                                                        'Three_Week_Prior_Weekly_Cases_Per',\n",
    "                                                        'Three_Week_Prior_Weekly_Cases',\n",
    "                                                        'Two_Week_Prior_Weekly_Cases_Per',\n",
    "                                                        'Two_Week_Prior_Weekly_Cases',\n",
    "                                                        'Three_Week_Prior_Weekly_Deaths_Per',\n",
    "                                                        'Three_Week_Prior_Weekly_Deaths',\n",
    "                                                        'Two_Week_Prior_Weekly_Deaths_Per',\n",
    "                                                        'Two_Week_Prior_Weekly_Deaths',                        \n",
    "                                                         'FIPS', 'Days']], how='inner', left_on=[\"fips\"], right_on=['FIPS'])\n",
    "#dropping NaN columns\n",
    "covariate_merged = covariates_merged.dropna(axis='columns')\n",
    "US_Deaths_Cases_df =  US_Deaths_Cases_df.dropna(axis='columns')\n",
    "fips_state=age_race_df[['STNAME','fips']]\n",
    "US_Deaths_Cases_df = US_Deaths_Cases_df.merge(fips_state, how='inner', left_on=[\"FIPS\"], right_on=['fips'])\n",
    "US_Deaths_Cases_df = US_Deaths_Cases_df.drop(columns=['index'])\n",
    "#pd.set_option('display.max_rows', None)\n",
    "#corr=covariate_merged.corr()\n",
    "#print(corr[['Cumulative_Deaths','Daily_Deaths']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#print(US_Deaths_Cases_df[((US_Deaths_Cases_df['Days'] == 221))]['Cumulative_Deaths'].sum())\n",
    "#print(US_Deaths_All_FIPS_df[((US_Deaths_All_FIPS_df['Days'] == 214))]['Cumulative_Deaths'].sum())\n",
    "#np.set_printoptions(suppress=True)\n",
    "#print(np.setdiff1d(US_Deaths_All_FIPS_df['FIPS'], US_Deaths_Cases_df['FIPS']))\n",
    "unassigned_fips = [  0.0, 60.0,    66.0,    69.0,    78.0,  2013.0,  2261.0, 72001.0,  72003.0,  72005.0,\n",
    " 72007.0,  72009.0,  72011.0,  72013.0,  72015.0,  72017.0,  72019.0,  72021.0,  72023.0,  72025.0,\n",
    " 72027.0,  72029.0,  72031.0,  72033.0,  72035.0,  72037.0,  72039.0,  72041.0,  72043.0,  72045.0,\n",
    " 72047.0,  72049.0,  72051.0,  72053.0,  72054.0,  72055.0,  72057.0,  72059.0,  72061.0,  72063.0,\n",
    " 72065.0,  72067.0,  72069.0,  72071.0,  72073.0,  72075.0,  72077.0,  72079.0,  72081.0,  72083.0,\n",
    " 72085.0,  72087.0,  72089.0,  72091.0,  72093.0,  72095.0,  72097.0,  72099.0,  72101.0,  72103.0,\n",
    " 72105.0,  72107.0,  72109.0,  72111.0,  72113.0,  72115.0,  72117.0,  72119.0,  72121.0,  72123.0,\n",
    " 72125.0,  72127.0,  72129.0,  72131.0,  72133.0,  72135.0,  72137.0,  72139.0,  72141.0,  72143.0,\n",
    " 72145.0,  72147.0,  72149.0,  72151.0,  72153.0,  72888.0,  72999.0,  80001.0, 80002.0, 80004.0, 80005.0,\n",
    " 80006.0, 80008.0, 80009.0, 80010.0, 80011.0, 80012.0, 80013.0, 80015.0, 80016.0, 80017.0,\n",
    " 80018.0, 80019.0, 80020.0, 80021.0, 80022.0, 80023.0, 80024.0, 80025.0, 80026.0, 80027.0,\n",
    " 80028.0, 80029.0, 80030.0, 80031.0, 80032.0, 80033.0, 80034.0, 80035.0, 80036.0, 80037.0,\n",
    " 80038.0, 80039.0, 80040.0, 80041.0, 80042.0, 80044.0, 80045.0, 80046.0, 80047.0, 80048.0,\n",
    " 80049.0, 80050.0, 80051.0, 80053.0, 80054.0, 80055.0, 80056.0, 88888.0, 90001.0, 90002.0,\n",
    " 90004.0, 90005.0, 90006.0, 90008.0, 90009.0, 90010.0, 90011.0, 90012.0, 90013.0, 90015.0,\n",
    " 90016.0, 90017.0, 90018.0, 90019.0, 90020.0, 90021.0, 90022.0, 90023.0, 90024.0, 90025.0,\n",
    " 90026.0, 90027.0, 90028.0, 90029.0, 90030.0, 90031.0, 90032.0, 90033.0, 90034.0, 90035.0,\n",
    " 90036.0, 90037.0, 90038.0, 90039.0, 90040.0, 90041.0, 90042.0, 90044.0, 90045.0, 90046.0,\n",
    " 90047.0, 90048.0, 90049.0, 90050.0, 90051.0, 90053.0, 90054.0, 90055.0, 90056.0, 99999.0]\n",
    "unassigned_deaths_df = US_Deaths_All_FIPS_df[US_Deaths_All_FIPS_df['FIPS'].isin(unassigned_fips)]\n",
    "US_unassigned_deaths = unassigned_deaths_df.groupby(['Province_State', 'Days'],as_index=False).agg({'Cumulative_Deaths': np.sum})\n",
    "US_unassigned_weekly_deaths = unassigned_deaths_df.groupby(['Province_State', 'Days'],as_index=False).agg({'Weekly_Deaths': np.sum})\n",
    "unassigned_deaths_by_days = unassigned_deaths_df.groupby(['Days'],as_index=False).agg({'Cumulative_Deaths': np.sum})\n",
    "unassigned_weekly_deaths_by_days = unassigned_deaths_df.groupby(['Days'],as_index=False).agg({'Weekly_Deaths': np.sum})\n",
    "US_unassigned_deaths_by_days = dict(zip(unassigned_deaths_by_days.Days,unassigned_deaths_by_days.Cumulative_Deaths))\n",
    "US_unassigned_weekly_deaths_by_days = dict(zip(unassigned_weekly_deaths_by_days.Days,unassigned_weekly_deaths_by_days.Weekly_Deaths))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(train_start_week_day,train_end_week_day,predict_start_week_day,predict_end_week_day,\n",
    "              covariates_merged,US_Deaths_Cases_df,predict_column_index,best_fit_scores):\n",
    "    predict_start_date = (covid_start_date + timedelta(days=predict_start_week_day)).date()\n",
    "    predict_end_date =  (covid_start_date + timedelta(days=predict_end_week_day)).date()\n",
    "    print(\"train:\", train_start_week_day ,\":\", train_end_week_day, \n",
    "        \" predict:\" , predict_start_week_day, \":\",  predict_end_week_day,\n",
    "        \" predict start date: \",  predict_start_date,\n",
    "        \" predict end date: \",  predict_end_date)\n",
    "    print \n",
    "    #print (\"training week number: \" , week_num+1)\n",
    "    #********************TRAIN MODEL ************************ \n",
    "    #train with all coutnies variates for one  week  and predict for LA county cumulative deaths\n",
    "    train_week = pd.Series(range(train_start_week_day,train_end_week_day))\n",
    "    #print (\"training week series :\" , train_week.array)\n",
    "    covariates_train_week = covariates_merged.loc[(covariates_merged['Days'].isin(train_week))] \n",
    "    US_Weekly_Cases_Deaths = US_Deaths_Cases_df.loc[(US_Deaths_Cases_df['Days'].isin(train_week))].iloc[:, predict_column_index]  \n",
    "    if len(US_Weekly_Cases_Deaths) > 0:\n",
    "       \n",
    "        X_train = covariates_train_week\n",
    "        Y_train = US_Weekly_Cases_Deaths\n",
    "        #reg = LassoCV(cv=5, random_state=0).fit(X_train, Y_train)\n",
    "        xgbReg = xgb.XGBRegressor()\n",
    "\n",
    "        parameters = {\n",
    "              'objective':['reg:squarederror'],\n",
    "              'learning_rate': [0.01, 0.05, 0.08], #so called `eta` value\n",
    "              'max_depth': [3,4],\n",
    "              'min_child_weight': [1,2],\n",
    "              'subsample': [0.8],\n",
    "              'colsample_bytree' : [0.8],\n",
    "              'n_estimators': [ 100,200]}\n",
    "        tscv = TimeSeriesSplit(n_splits=4)\n",
    "        xgb_grid = GridSearchCV(xgbReg,\n",
    "                        parameters,\n",
    "                        cv = tscv,\n",
    "                        n_jobs = 4,\n",
    "                        scoring = 'r2',\n",
    "                        verbose=True)\n",
    "\n",
    "        xgb_grid.fit(X_train,Y_train)\n",
    "\n",
    "\n",
    "        print(xgb_grid.best_score_)\n",
    "        print(xgb_grid.best_params_)\n",
    "        results = xgb_grid.cv_results_\n",
    "        predict_week = pd.Series(range(predict_start_week_day, predict_end_week_day ))\n",
    "        #print (\"predicting next week series :\" , predict_week.array)\n",
    "\n",
    "        X_predict_week = covariates_merged.loc[(covariates_merged['Days'].isin(predict_week))]\n",
    "        predict_week_df = US_Deaths_Cases_df.loc[(US_Deaths_Cases_df['Days'].isin(predict_week))]\n",
    "        Y_actual_predict_week = predict_week_df.iloc[:,predict_column_index]\n",
    "        #best_reg.fit(X_tune_week, Y_actual_tune_week)\n",
    "        Y_predict_week = xgb_grid.best_estimator_.predict(X_predict_week)\n",
    "        predict_score = r2_score(Y_actual_predict_week, Y_predict_week)\n",
    "        print(week_num , \":\", predict_score)\n",
    "        best_fit_scores.loc[index] = [str(predict_start_date) + ' - '  + str(predict_end_date), xgb_grid.best_params_, xgb_grid.best_score_,  predict_score]\n",
    "\n",
    "        return Y_predict_week\n",
    "    return None\n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "id": "WghtKSQ6xAYp",
    "outputId": "8b2721d6-8f6d-4812-84ef-7592ea4dda37",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_day:  0  max day:  284  number of weeks :  40\n",
      "train: 236 : 257  predict: 257 : 285\n",
      "train: 236 : 257  predict: 257 : 285  predict start date:  2020-10-05  predict end date:  2020-11-02\n",
      "Fitting 4 folds for each of 24 candidates, totalling 96 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:  3.5min\n",
      "[Parallel(n_jobs=4)]: Done  96 out of  96 | elapsed:  8.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5777970642330387\n",
      "{'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 3, 'min_child_weight': 2, 'n_estimators': 200, 'objective': 'reg:squarederror', 'subsample': 0.8}\n",
      "0 : 0.8698148509152076\n",
      "train: 236 : 257  predict: 257 : 285  predict start date:  2020-10-05  predict end date:  2020-11-02\n",
      "Fitting 4 folds for each of 24 candidates, totalling 96 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=4)]: Done  96 out of  96 | elapsed:  8.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6764015597306707\n",
      "{'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 200, 'objective': 'reg:squarederror', 'subsample': 0.8}\n",
      "0 : 0.7899200097926478\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import MultiTaskLassoCV\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "import numpy as np\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import xgboost as xgb\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import GridSearchCV,TimeSeriesSplit\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "covid_start_date = datetime.strptime(\"01/22/20\", \"%m/%d/%y\")\n",
    "\n",
    "\n",
    "min_day=US_Deaths_Cases_df['Days'].min()\n",
    "max_day=US_Deaths_Cases_df['Days'].max()\n",
    "countyFIPS = US_Deaths_Cases_df[\"FIPS\"].unique()\n",
    "\n",
    "num_weeks=int((max_day-min_day)/7)\n",
    "print(\"min_day: \" , min_day, \" max day: \", max_day, \" number of weeks : \" , num_weeks)\n",
    "#prediction_days_arr = [7,14,21,28]\n",
    "#prediction_days = 7   # one week  train , tune and forecast\n",
    "#prediction_days = 14 # two weks train , tune and forecast\n",
    "prediction_days = 21 # three weks train , tune and forecast\n",
    "#prediction_days = 28 # four weeks train , tune and forecast\n",
    "\n",
    "best_fit_scores = pd.DataFrame(columns=[\"Week Duration\", \"Best_Parameters\", \"Best_Score\", \"Predict_R2_Score\"])\n",
    "predicted_df_all_days = pd.DataFrame(columns=['State','FIPS', 'Population','Forecast_Day','Days','Predicted_Weekly_Deaths' 'Predicted_Cumulative_Deaths','Predicted_Weekly_Cases' 'Predicted_Cumulative_Cases''Past_Week_Cumulative_Deaths'])\n",
    "\n",
    "train_start_week_day = model_day_offset\n",
    "train_end_week_day = train_start_week_day + prediction_days\n",
    "\n",
    "predict_start_week_day = train_end_week_day \n",
    "predict_end_week_day = predict_start_week_day + 28\n",
    "\n",
    "print(\"train:\", train_start_week_day ,\":\", train_end_week_day, \n",
    "        \" predict:\" , predict_start_week_day, \":\",  predict_end_week_day)\n",
    "\n",
    "for week_num in range(num_weeks):\n",
    "    if (predict_end_week_day-1) > max_day:\n",
    "        break\n",
    "    predicted_df = pd.DataFrame(columns=['State','FIPS','Population', 'Forecast_Day','Days','Predicted_Weekly_Deaths', 'Predicted_Weekly_Cases', 'Predicted_Cumulative_Deaths', 'Predicted_Cumulative_Cases','Past_Week_Cumulative_Deaths'])        \n",
    "    predicted_deaths = run_model(train_start_week_day,train_end_week_day,predict_start_week_day,predict_end_week_day,\n",
    "              covariates_merged,US_Deaths_Cases_df,5,best_fit_scores) \n",
    "    predicted_cases = run_model(train_start_week_day,train_end_week_day,predict_start_week_day,predict_end_week_day,\n",
    "              covariates_merged,US_Deaths_Cases_df,10,best_fit_scores) \n",
    "    predict_week = pd.Series(range(predict_start_week_day, predict_end_week_day ))\n",
    "    predict_week_df = US_Deaths_Cases_df.loc[(US_Deaths_Cases_df['Days'].isin(predict_week))]\n",
    "    predicted_df[\"State\"] = predict_week_df[\"STNAME\"]\n",
    "    predicted_df[\"FIPS\"] = predict_week_df[\"FIPS\"]\n",
    "    predicted_df[\"Population\"] = predict_week_df[\"Population\"]\n",
    "    predicted_df[\"Days\"] = predict_week_df[\"Days\"]\n",
    "    forecast_day = (covid_start_date + timedelta(days=(predict_start_week_day-1))).strftime(\"%Y-%m-%d\")\n",
    "    predicted_df[\"Forecast_Day\"] = forecast_day \n",
    "    predicted_df[\"Predicted_Weekly_Deaths\"] = predicted_deaths\n",
    "    predicted_df[\"Predicted_Weekly_Cases\"]  = predicted_cases\n",
    "                                         \n",
    "    predicted_df[\"Predicted_Cumulative_Deaths\"] = predicted_df[\"Predicted_Weekly_Deaths\"] + predict_week_df[\"Past_Week_Cumulative_Deaths\"]\n",
    "    predicted_df[\"Predicted_Cumulative_Cases\"] = predicted_df[\"Predicted_Weekly_Cases\"] + predict_week_df[\"Past_Week_Cumulative_Cases\"]\n",
    "    predicted_df['Past_Week_Cumulative_Deaths'] = predict_week_df[\"Past_Week_Cumulative_Deaths\"]\n",
    "    pwcdArray = predicted_df[\"Predicted_Cumulative_Deaths\"].to_numpy()\n",
    "    pwccArray = predicted_df[\"Predicted_Cumulative_Cases\"].to_numpy()\n",
    "    fipsArray = predicted_df['FIPS'].to_numpy()\n",
    "    j = 0\n",
    "    i = 0 \n",
    "    FIPS = 0.0\n",
    "    for index, row in predicted_df.iterrows():\n",
    "        countyFIPS = row['FIPS']\n",
    "        if  FIPS == countyFIPS:\n",
    "            if (j > 6) and  fipsArray[i-7] == countyFIPS:\n",
    "                past_week_cum_deaths = pwcdArray[i-7]\n",
    "                past_week_cum_cases= pwccArray[i-7]\n",
    "                predicted_df.at[index, 'Predicted_Cumulative_Deaths'] = (row[\"Predicted_Weekly_Deaths\"]) + past_week_cum_deaths\n",
    "                predicted_df.at[index, 'Predicted_Cumulative_Cases'] = (row[\"Predicted_Weekly_Cases\"]) + past_week_cum_cases\n",
    "                predicted_df.at[index,'Past_Week_Cumulative_Deaths'] = past_week_cum_deaths                \n",
    "                pwcdArray[i] = predicted_df.at[index, 'Predicted_Cumulative_Deaths']\n",
    "                pwccArray[i] = predicted_df.at[index, 'Predicted_Cumulative_Cases']\n",
    "                \n",
    "        else:\n",
    "            FIPS = row['FIPS']\n",
    "            j = 0 \n",
    "        i = i + 1\n",
    "        j = j + 1\n",
    "    predicted_df.to_csv(\"data/predictions/predicted_us_deaths_\" +  forecast_day + \".csv\")                     \n",
    "    predicted_df_all_days = pd.concat([predicted_df_all_days, predicted_df])\n",
    "\n",
    "    train_start_week_day = train_start_week_day + 7\n",
    "    train_end_week_day = train_end_week_day + 7\n",
    "    predict_start_week_day = train_end_week_day\n",
    "    predict_end_week_day = predict_start_week_day + 28\n",
    "   \n",
    "   \n",
    "predicted_df_all_days.to_csv(\"predicted_all_days.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "S7woMnPEfGvn",
    "outputId": "2c83fed8-edf8-439a-aa78-b21440f8a386"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-18-7aa744923adc>:6: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "  state_predicted_deaths = predicted_df_all_days.groupby(['State', 'Days', 'Forecast_Day'], as_index=False)['Predicted_Cumulative_Deaths','Predicted_Weekly_Deaths'].sum()\n",
      "<ipython-input-18-7aa744923adc>:10: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "  country_predicted_deaths = predicted_df_all_days.groupby(['Days', 'Forecast_Day'], as_index=False)['Predicted_Cumulative_Deaths','Predicted_Weekly_Deaths'].sum()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-04\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "#predicted_df_all_days = pd.read_csv(\"predicted_all_days.csv\")\n",
    "covid_start_date = datetime.strptime(\"01/22/20\", \"%m/%d/%y\")\n",
    "\n",
    "state_predicted_deaths = predicted_df_all_days.groupby(['State', 'Days', 'Forecast_Day'], as_index=False)['Predicted_Cumulative_Deaths','Predicted_Weekly_Deaths'].sum()\n",
    "state_to_fips=pd.read_csv('https://docs.google.com/spreadsheets/d/1w4sHgYifJV-C8J1WV5rpTzEFLuyZ5ESp8r_1ck-9hlA/export?format=csv')\n",
    "state_predicted_deaths = state_predicted_deaths.merge(state_to_fips, how='left', left_on=['State'], right_on=['Name'])\n",
    "state_predicted_deaths=state_predicted_deaths.drop(columns=['Name'])\n",
    "country_predicted_deaths = predicted_df_all_days.groupby(['Days', 'Forecast_Day'], as_index=False)['Predicted_Cumulative_Deaths','Predicted_Weekly_Deaths'].sum()\n",
    "                        \n",
    "\n",
    "state_predicted_deaths=state_predicted_deaths[['State','FIPS','Days','Forecast_Day','Predicted_Cumulative_Deaths','Predicted_Weekly_Deaths']]\n",
    "iteration = 0  \n",
    "forecast_dates = country_predicted_deaths[\"Forecast_Day\"].unique()\n",
    "for forecast_date in forecast_dates:\n",
    "    print(forecast_date)  \n",
    "    covid_hub_predicted_deaths = pd.DataFrame(columns=['forecast_date','target', 'target_end_date','location','type', 'quantile', 'value']) \n",
    "    filename = \"forecast-hub/data-processed/MIT_CritData-GBCF/{}-MIT_CritData-GBCF.csv\".format(forecast_date)\n",
    "    cp_detahs_forecast= country_predicted_deaths[country_predicted_deaths[\"Forecast_Day\"] == forecast_date]\n",
    "    max_day = US_unassigned_deaths['Days'].max()  \n",
    "    for index, row in cp_detahs_forecast.iterrows():\n",
    "\n",
    "      #predicted_df[\"FIPS\"] = predict_week_df[\"FIPS\"]\n",
    "      #predicted_df[\"Days\"] = predict_week_df[\"Days\"]\n",
    "      target_date = (covid_start_date + timedelta(days=row[\"Days\"]))\n",
    "      if target_date.weekday() == 5:\n",
    "          covid_hub_predicted_deaths.loc[iteration,\"location\"] = 'US'\n",
    "          #covid_hub_predicted_deaths.loc[iteration,\"location_name\"] = 'US'\n",
    "\n",
    "          forecast_day =  (datetime.strptime(row[\"Forecast_Day\"],\"%Y-%m-%d\") - covid_start_date).days\n",
    "          days_from = str(row[\"Days\"] - forecast_day)\n",
    "          actual_week =  (int(days_from) + 1)/7\n",
    "          covid_hub_predicted_deaths.loc[iteration,\"target\"] = str(int(actual_week)) + \" wk ahead cum death\"\n",
    "          covid_hub_predicted_deaths.loc[iteration,\"target_end_date\"] = target_date.strftime(\"%Y-%m-%d\")\n",
    "          covid_hub_predicted_deaths.loc[iteration,\"forecast_date\"] =  row[\"Forecast_Day\"]\n",
    "          covid_hub_predicted_deaths.loc[iteration,\"type\"] =\"point\"\n",
    "          covid_hub_predicted_deaths.loc[iteration,\"quantile\"] =\"NA\"\n",
    "          if row[\"Predicted_Cumulative_Deaths\"] >= 0:\n",
    "\n",
    "            if row['Days'] in US_unassigned_deaths_by_days.keys():\n",
    "                covid_hub_predicted_deaths.loc[iteration,\"value\"] = int(round(row[\"Predicted_Cumulative_Deaths\"] + US_unassigned_deaths_by_days[row['Days']-7]))\n",
    "            else:\n",
    "                covid_hub_predicted_deaths.loc[iteration,\"value\"] = int(round(row[\"Predicted_Cumulative_Deaths\"] + US_unassigned_deaths_by_days[max_day]))\n",
    "          else:\n",
    "            covid_hub_predicted_deaths.loc[iteration,\"value\"] = 0\n",
    "\n",
    "          iteration = iteration + 1\n",
    "\n",
    "    for index, row in cp_detahs_forecast.iterrows():\n",
    "\n",
    "      #predicted_df[\"FIPS\"] = predict_week_df[\"FIPS\"]\n",
    "      #predicted_df[\"Days\"] = predict_week_df[\"Days\"]\n",
    "      target_date = (covid_start_date + timedelta(days=row[\"Days\"]))\n",
    "      if target_date.weekday() == 5:\n",
    "          covid_hub_predicted_deaths.loc[iteration,\"location\"] = 'US'\n",
    "          #covid_hub_predicted_deaths.loc[iteration,\"location_name\"] = 'US'\n",
    "\n",
    "          forecast_day =  (datetime.strptime(row[\"Forecast_Day\"],\"%Y-%m-%d\") - covid_start_date).days\n",
    "          days_from = str(row[\"Days\"] - forecast_day)\n",
    "          actual_week =  (int(days_from) + 1)/7\n",
    "          covid_hub_predicted_deaths.loc[iteration,\"target\"] = str(int(actual_week)) + \" wk ahead inc death\"\n",
    "          covid_hub_predicted_deaths.loc[iteration,\"target_end_date\"] = target_date.strftime(\"%Y-%m-%d\")\n",
    "          covid_hub_predicted_deaths.loc[iteration,\"forecast_date\"] =  row[\"Forecast_Day\"]\n",
    "          covid_hub_predicted_deaths.loc[iteration,\"type\"] =\"point\"\n",
    "          covid_hub_predicted_deaths.loc[iteration,\"quantile\"] =\"NA\"\n",
    "          if row[\"Predicted_Cumulative_Deaths\"] >= 0:\n",
    "\n",
    "            if row['Days'] in US_unassigned_deaths_by_days.keys():\n",
    "                covid_hub_predicted_deaths.loc[iteration,\"value\"] = int(round(row[\"Predicted_Weekly_Deaths\"] +  US_unassigned_weekly_deaths_by_days[row['Days']])) \n",
    "            else:\n",
    "                covid_hub_predicted_deaths.loc[iteration,\"value\"] = int(round(row[\"Predicted_Weekly_Deaths\"] +  US_unassigned_weekly_deaths_by_days[max_day])) \n",
    "          else:\n",
    "            covid_hub_predicted_deaths.loc[iteration,\"value\"] = 0\n",
    "\n",
    "          iteration = iteration + 1\n",
    "\n",
    "    state_predicted_deaths_forecast = state_predicted_deaths[state_predicted_deaths[\"Forecast_Day\"] == forecast_date]\n",
    "    for index, row in state_predicted_deaths_forecast.iterrows():\n",
    "\n",
    "      target_date = (covid_start_date + timedelta(days=row[\"Days\"]))\n",
    "      if target_date.weekday() == 5:\n",
    "          covid_hub_predicted_deaths.loc[iteration,\"location\"] = str(row[\"FIPS\"]).zfill(2)\n",
    "          #covid_hub_predicted_deaths.loc[iteration,\"location_name\"] = row[\"State\"]\n",
    "\n",
    "          forecast_day =  (datetime.strptime(row[\"Forecast_Day\"],\"%Y-%m-%d\") - covid_start_date).days\n",
    "          days_from = str(row[\"Days\"] - forecast_day)\n",
    "          actual_week =  (int(days_from) + 1)/7\n",
    "          covid_hub_predicted_deaths.loc[iteration,\"target\"] = str(int(actual_week)) + \" wk ahead cum death\"\n",
    "          covid_hub_predicted_deaths.loc[iteration,\"target_end_date\"] = target_date.strftime(\"%Y-%m-%d\")\n",
    "          covid_hub_predicted_deaths.loc[iteration,\"forecast_date\"] =  row[\"Forecast_Day\"]\n",
    "          covid_hub_predicted_deaths.loc[iteration,\"type\"] =\"point\"\n",
    "          covid_hub_predicted_deaths.loc[iteration,\"quantile\"] =\"NA\"\n",
    "          if row[\"Predicted_Cumulative_Deaths\"] >= 0:\n",
    "                days = row['Days']\n",
    "                max_day = US_unassigned_deaths['Days'].max()\n",
    "                if (days <= max_day):                \n",
    "                    covid_hub_predicted_deaths.loc[iteration,\"value\"] = int(round(row[\"Predicted_Cumulative_Deaths\"] + US_unassigned_deaths[(US_unassigned_deaths['Province_State'] == row['State']) & (US_unassigned_deaths['Days'] == row['Days']-7)]['Cumulative_Deaths'].values[0]))\n",
    "                else:\n",
    "                    covid_hub_predicted_deaths.loc[iteration,\"value\"] = int(round(row[\"Predicted_Cumulative_Deaths\"] + US_unassigned_deaths[(US_unassigned_deaths['Province_State'] == row['State']) & (US_unassigned_deaths['Days'] == max_day)]['Cumulative_Deaths'].values[0]))       \n",
    "          else:\n",
    "            covid_hub_predicted_deaths.loc[iteration,\"value\"] = 0\n",
    "          iteration = iteration + 1\n",
    "\n",
    "    for index, row in state_predicted_deaths_forecast.iterrows():\n",
    "\n",
    "      target_date = (covid_start_date + timedelta(days=row[\"Days\"]))\n",
    "      if target_date.weekday() == 5:\n",
    "          covid_hub_predicted_deaths.loc[iteration,\"location\"] = str(row[\"FIPS\"]).zfill(2)\n",
    "          #covid_hub_predicted_deaths.loc[iteration,\"location_name\"] = row[\"State\"]\n",
    "\n",
    "          forecast_day =  (datetime.strptime(row[\"Forecast_Day\"],\"%Y-%m-%d\") - covid_start_date).days\n",
    "          days_from = str(row[\"Days\"] - forecast_day)\n",
    "          actual_week =  (int(days_from) + 1)/7\n",
    "          covid_hub_predicted_deaths.loc[iteration,\"target\"] = str(int(actual_week)) + \" wk ahead inc death\"\n",
    "          covid_hub_predicted_deaths.loc[iteration,\"target_end_date\"] = target_date.strftime(\"%Y-%m-%d\")\n",
    "          covid_hub_predicted_deaths.loc[iteration,\"forecast_date\"] =  row[\"Forecast_Day\"]\n",
    "          covid_hub_predicted_deaths.loc[iteration,\"type\"] =\"point\"\n",
    "          covid_hub_predicted_deaths.loc[iteration,\"quantile\"] =\"NA\"\n",
    "          if row[\"Predicted_Cumulative_Deaths\"] >= 0:\n",
    "                days = row['Days']\n",
    "                max_day = US_unassigned_deaths['Days'].max()\n",
    "                if (days <= max_day):                \n",
    "                    wd = int(round(row[\"Predicted_Weekly_Deaths\"] + US_unassigned_weekly_deaths[(US_unassigned_weekly_deaths['Province_State'] == row['State']) & (US_unassigned_weekly_deaths['Days'] == row['Days'])]['Weekly_Deaths'].values[0]))\n",
    "                else:\n",
    "                    wd = int(round(row[\"Predicted_Weekly_Deaths\"] + US_unassigned_weekly_deaths[(US_unassigned_weekly_deaths['Province_State'] == row['State']) & (US_unassigned_weekly_deaths['Days'] == max_day)]['Weekly_Deaths'].values[0]))       \n",
    "                if wd < 0:\n",
    "                        covid_hub_predicted_deaths.loc[iteration,\"value\"] = 0\n",
    "                else:\n",
    "                    covid_hub_predicted_deaths.loc[iteration,\"value\"] = wd    \n",
    "          else:\n",
    "            covid_hub_predicted_deaths.loc[iteration,\"value\"] = 0\n",
    "          iteration = iteration + 1\n",
    "\n",
    "    covid_hub_predicted_deaths.to_csv(filename,index = False)\n",
    "  #files.download(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate_models\n",
    "from datetime import datetime, timedelta\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "files = ['2020-08-30-MIT_CritData-GBCF.csv'] #listdir('forecast-hub/data-processed/MIT_CritData-GBCF')\n",
    "files.sort()\n",
    "for f in files :\n",
    "    if f!= '.DS_Store':\n",
    "        proj_date = datetime.strptime(f[:10],\"%Y-%m-%d\").date() +  timedelta(days=1) #Monday\n",
    "        eval_date = proj_date +  timedelta(days=5) #Saturday\n",
    "\n",
    "        print(\"running evaluation for proj_date: \", proj_date, \" eval_date: \" , eval_date)\n",
    "        evaluate_models.run_evaluation('forecast-hub',proj_date, eval_date, \"eval/one-week\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for f in files :\n",
    "    if f!= '.DS_Store':\n",
    "        proj_date = datetime.strptime(f[:10],\"%Y-%m-%d\").date() +  timedelta(days=1) #Monday\n",
    "        eval_date = proj_date +  timedelta(days=12) #Saturday\n",
    "\n",
    "        print(\"running evaluation for proj_date: \", proj_date, \" eval_date: \" , eval_date)\n",
    "        evaluate_models.run_evaluation('forecast-hub',proj_date, eval_date, \"eval/two-weeks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Forecaster v2",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
